<!DOCTYPE HTML>
<html lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<!-- Please don't forget to delete the two Google Analytics script tags if you use this HTML.-->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-147961896-1"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());
			gtag('config', 'UA-147961896-1');
		</script>
		<title>Ahmed Rida SEKKAT</title>
		<meta name="author" content="Ahmed Rida SEKKAT">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="../stylesheet.css">
		<link rel="icon" type="image/png" href="https://drive.google.com/uc?export=view&id=1Gll0pAIAkcY5fqMJAHCg_tU3OG75aTzx">
	</head>
	<body bgcolor="#DCDCDC">
		<br/>
		<table bgcolor="#F8F8FF" style="box-shadow:0 4px 8px 0 grey, 0 6px 20px 0 grey;border-radius:5px;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tr style="padding:0px">
				<td style="padding:0px">
					<table style="cursor:pointer;height:50px;width:100%;max-width:1000px;border-color:#D3D3D3;border:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
						<tr>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='../index.html';" width="12.5%"><a href="../index.html" style="display:block">About me</a></th>
							<th style="border-radius:5px 5px 0px 0px" bgcolor="#F8F8FF" onclick="window.location='research.html';" width="12.5%"><a href="research.html" style="display:block">Research</a></th>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='education.html';" width="12.5%"><a href="education.html" style="display:block">Education</a></th>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='experience.html';" width="12.5%"><a href="experience.html" style="display:block">Experience</a></th>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='talks.html';" width="12.5%"><a href="talks.html" style="display:block">Talks</a></th>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='cv.html';" width="12.5%"><a href="cv.html" style="display:block">CV</a></th>
							<th class="tab" style="border-radius:5px 5px 0px 0px" bgcolor="#D3D3D3" onclick="window.location='miscellaneous.html';" width="12.5%"><a href="miscellaneous.html" style="display:block">Miscellaneous</a></th>
						</tr> 
					</table>
				</td>
			</tr>
			<tr>
				<td class="exptable_mobile exptable_desk">
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('ScientificReports2022_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="ScientificReports2022">
						<tr onmouseover="highlight('ScientificReports2022')" onmouseout="lowlight('ScientificReports2022')">
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px;height:125px" src='images/ScientificReports2022.png' id="ScientificReports2022.png">
								<img style="display:none;border-radius:5px;width:125px;height:125px" src='images/ScientificReports2022.gif' id="ScientificReports2022.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="" target="_blank"> -->
										<strong><font size="3">A comparative study of semantic segmentation of omnidirectional images from a motorcycle perspective</strong></font>
									<!-- </a> -->
									<br/>
									<strong>Ahmed Rida Sekkat</strong>,
									Yohan Dupuis,
									Paul Honeine,
									Pascal Vasseur.
									<br/>
									<i>Scientific Reports 12, 4968 (2022).</i>
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="ScientificReports2022_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="ScientificReports2022_td">
								<table>
									<tr>
										<td style="vertical-align:top">
											<p style="text-align: justify">
												<strong>Abstract:</strong> The semantic segmentation of omnidirectional urban driving images is a research topic that has increasingly attracted the
																			attention of researchers, because the use of such images in driving scenes is highly relevant. However, the case of motorized
																			two-wheelers has not been treated yet. Since the dynamics of these vehicles are very different from those of cars, we focus
																			our study on images acquired using a motorcycle. This paper provides a thorough comparative study to show how different
																			deep learning approaches handle omnidirectional images with different representations, including perspective, equirectangular,
																			spherical, and fisheye, and presents the best solution to segment road scene omnidirectional images. We use in this study real
																			perspective images, and synthetic perspective, fisheye and equirectangular images, simulated fisheye images, as well as a test
																			set of real fisheye images. By analyzing both qualitative and quantitative results, the conclusions of this study are multiple, as
																			it helps understand how the networks learn to deal with omnidirectional distortions. Our main findings are that models with
																			planar convolutions give better results than the ones with spherical convolutions, and that models trained on omnidirectional
																			representations transfer better to standard perspective images than vice versa.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://rdcu.be/cJC4j" target="_blank">Paper</a>]
												[<a href="https://doi.org/10.1038/s41598-022-08466-9" target="_blank">HTML</a>]
												[<a href="https://www.nature.com/articles/s41598-022-08466-9.pdf" target="_blank">PDF</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('SynWoodScape_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="SynWoodScape">
						<tr onmouseover="highlight('SynWoodScape')" onmouseout="lowlight('SynWoodScape')">
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;width:150px;height:84px" src='images/SynWoodScape.png' id="SynWoodScape.png">
								<img style="display:none;width:150px;height:84px" src='images/SynWoodScape.gif' id="SynWoodScape.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="" target="_blank"> -->
										<strong><font size="3">SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving</strong></font>
									<!-- </a> -->
									<br/>
									<strong>Ahmed Rida Sekkat</strong>,
									Yohan Dupuis,
									Varun Ravi Kumar,
									Hazem Rashed,
									Senthil Yogamani,
									Pascal Vasseur,
									Paul Honeine.
									<br/>
									<i>arXiv preprint arXiv:2203.05056, 2022/3/9, (under review).</i>
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="SynWoodScape_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="SynWoodScape_td">
								<table>
									<tr>
										<td style="vertical-align:top">
											<p style="text-align: justify">
												<strong>Abstract:</strong> Surround-view cameras are a primary sensor for
																		automated driving, used for near field perception. It is one
																		of the most commonly used sensors in commercial vehicles.
																		Four fisheye cameras with a 190° field of view cover the 360°
																		around the vehicle. Due to its high radial distortion, the standard
																		algorithms do not extend easily. Previously, we released the first
																		public fisheye surround-view dataset named WoodScape. In this
																		work, we release a synthetic version of the surround-view dataset,
																		covering many of its weaknesses and extending it. Firstly, it is
																		not possible to obtain ground truth for pixel-wise optical flow
																		and depth. Secondly, WoodScape did not have all four cameras
																		simultaneously in order to sample diverse frames. However, this
																		means that multi-camera algorithms cannot be designed, which
																		is enabled in the new dataset. We implemented surround-view
																		fisheye geometric projections in CARLA Simulator matching
																		WoodScape’s configuration and created SynWoodScape. We release
																		80k images from the synthetic dataset with annotations
																		for 10+ tasks. We also release the baseline code and supporting scripts.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://arxiv.org/abs/2203.05056" target="_blank">Paper</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('RFIAP20_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="RFIAP20">
						<tr onmouseover="highlight('RFIAP20')" onmouseout="lowlight('RFIAP20')">
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px;height:125px" src='images/RFIAP20.png' id="RFIAP20.png">
								<img style="display:none;border-radius:5px;width:125px;height:125px" src='images/RFIAP20.gif' id="RFIAP20.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="https://hal-normandie-univ.archives-ouvertes.fr/hal-02183033/document" target="_blank"> -->
										<strong><font size="3">A comparative study of semantic segmentation using omnidirectional images</font></strong>
									<!-- </a> -->
									<br/>
									<strong>Ahmed Rida Sekkat</strong>,
									Yohan Dupuis,
									Paul Honeine,
									Pascal Vasseur.
									<br/>
									<i>Congrès Reconnaissance des Formes, Image, Apprentissage et Perception (RFIAP 2020), Vannes, Bretagne, France, 23 - 26 June 2020.</i>
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="RFIAP20_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="RFIAP20_td">
								<table>
									<tr>
										<td style="vertical-align:top">
											<p style="text-align: justify">
												<strong>Abstract:</strong> The semantic segmentation of omnidirectional urban driving images is a research 
												topic that has increasingly attracted the attention of researchers. This paper presents a thorough comparative 
												study of different neural network models trained on four different representations: perspective, 
												equirectangular, spherical and fisheye. We use in this study real perspective images, and synthetic 
												perspective, fisheye and equirectangular images, as well as a test set of real fisheye images. 
												We evaluate the performance of convolution on spherical images and perspective images. 
												The conclusions obtained by analyzing the results of this study are multiple and help understanding 
												how different networks learn to deal with omnidirectional distortions. Our main finding is that models 
												trained on omnidirectional images are robust against modality changes and are able to learn a universal 
												representation, giving good results in both perspective and omnidirectional images. The relevance of all 
												results is examined with an analysis of quantitative measures.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://cap-rfiap2020.sciencesconf.org/data/RFIAP_2020_paper_47.pdf" target="_blank">Paper</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('omniscape_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="omniscape">
						<tr  onmouseover="highlight('omniscape')" onmouseout="lowlight('omniscape')">
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px;height:110px" src='images/omniscape.png' id="omniscape.png">
								<img style="display:none;border-radius:5px;width:125px;height:110px" src='images/omniscape.gif' id="omniscape.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="https://github.com/ARSekkat/OmniScape" target="_blank"> -->
										<strong><font size="3">The OmniScape Dataset</strong></font>
									<!-- </a> -->
									<br/>
									<strong>Ahmed Rida Sekkat</strong>,
									Yohan Dupuis,
									Pascal Vasseur,
									Paul Honeine.
									<br/>
									<!--  <font style="opacity:0">.</font> -->
								    <i>IEEE International Conference on Robotics and Automation (ICRA), 2020.</i> 
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="omniscape_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="omniscape_td">
								<table>
									<tr>
										<td style="vertical-align:middle">
											<p style="text-align: justify">
												<strong>Abstract:</strong> Despite the utility and benefits of omnidirectional images in robotics and automotive applications, there are no datasets 
												of omnidirectional images available with semantic segmentation, depth map, and dynamic properties. This is due to the time cost and human effort required 
												to annotate ground truth images. This paper presents a framework for generating omnidirectional images using images that are acquired from a virtual 
												environment. For this purpose, we demonstrate the relevance of the proposed framework on two well-known simulators: CARLA simulator, which is an 
												open-source simulator for autonomous driving research, and Grand Theft Auto V (GTA V), which is a very high quality video game. We explain in details 
												the generated OmniScape dataset, which includes stereo fisheye and catadioptric images acquired from the two front sides of a motorcycle, including 
												semantic segmentation, depth map, intrinsic parameters of the cameras and the dynamic parameters of the motorcycle. It is worth noting that the case 
												of two-wheeled vehicles is more challenging than cars due to the specific dynamic of these vehicles.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://drive.google.com/file/d/1fb7tvN-5vI1LlBswMyEKA8ckaKcDHrIF/preview" target="_blank">Paper</a>]
												<br/>
												[<a href="https://www.youtube.com/watch?v=k5LdjaF5cvg" target="_blank">Presentation</a>]
												<br/>
												[<a href="https://github.com/ARSekkat/OmniScape" target="_blank">GitHub</a>]
												<br/>
												[<a href="https://www.youtube.com/playlist?list=PL4jFAx3iziLO9If7esOYJse2A86L68UBi" target="_blank">Demos</a>]
												<br/>
												[<a href="../omniscape/index.html" target="_blank">Project</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('gretsi19_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="gretsi19">
						<tr onmouseover="highlight('gretsi19')" onmouseout="lowlight('gretsi19')">
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px;height:125px" src='images/gretsi19.png' id="gretsi19.png">
								<img style="display:none;border-radius:5px;width:125px;height:125px" src='images/gretsi19.gif' id="gretsi19.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="https://hal-normandie-univ.archives-ouvertes.fr/hal-02183033/document" target="_blank"> -->
										<strong><font size="3">Génération d'images omnidirectionnelles à partir d'un environnement virtuel</strong></font>
									<!-- </a> -->
									<br/>
									<strong>Ahmed Rida Sekkat</strong>,
									Yohan Dupuis,
									Pascal Vasseur,
									Paul Honeine.
									<br/>
									<i>GRETSI, 2019 27-ème Colloque GRETSI sur le Traitement du Signal et des Images, Aug 2019, Lille, France.</i>
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="gretsi19_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="gretsi19_td">
								<table>
									<tr>
										<td style="vertical-align:top">
											<p style="text-align: justify">
												<strong>Abstract:</strong> This paper describes a method for generating omnidirectional images using cubemap images and corresponding 
												depth maps that can be acquired from a virtual environment. For this purpose, we use the video game Grand Theft Auto V (GTA V). GTA V 
												has been used as a data source in many research projects, due to the fact that it is a hyperrealist open-world game that simulates a 
												real city. We take advantage of developments made in reverse engineering this game, in order to extract realistic images and corresponding 
												depth maps using virtual cameras with 6DoF. By combining the extracted information with an omnidirectional camera model, we generate Fish-eye 
												images intended for instance to machine learning based applications.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://hal-normandie-univ.archives-ouvertes.fr/hal-02183033/document" target="_blank">Paper</a>]
												<br/>
												[<a href="https://drive.google.com/file/d/198EMNieHS-ENYrtT2zVpW20URCsvT30D/view" target="_blank">Poster</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('mpeg_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="mpeg">
						<tr onmouseover="highlight('mpeg')" onmouseout="lowlight('mpeg')" >
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px" src='https://mpeg.chiariglione.org/sites/default/files/mpeg-logo.png' id="mpeg.png">
								<img style="display:none;border-radius:5px;width:125px" src='https://mpeg.chiariglione.org/sites/default/files/mpeg-logo.png' id="mpeg.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="https://drive.google.com/file/d/1gBrmGWp9RgMzzXh5VeqqnyCKKYDzp41U/preview" target="_blank"> -->
										<strong><font size="3">Input to MV-HEVC LHEVCFF conformance</strong></font>
									<!-- </a> -->
									<br/>
									Jean  Le Feuvre,
									<strong>Ahmed Rida Sekkat</strong>.
									<br/>
									<i>MPEG, Chengdu, China, October 2016, n° m39261.</i> 
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="mpeg_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="mpeg_td">
								<table>
									<tr>
										<td style="vertical-align:middle">
											<p style="text-align: justify">
												MPEG standardization contribution: This contribution proposes a set of files for conformance for the Layered HEVC File Format for MV-HEVC bitstreams.
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://drive.google.com/file/d/1gBrmGWp9RgMzzXh5VeqqnyCKKYDzp41U/preview" target="_blank">Paper</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
					<table class="hoverTable" onclick="show_hide_abstract('ts15_abstract')" style="border-radius:5px;cursor:pointer;width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="ts15">
						<tr onmouseover="highlight('ts15')" onmouseout="lowlight('ts15')" >
							<td align="center" style="width:10%;min-width:155px;border-radius:5px 0px 0px 5px;vertical-align:middle">
								<img style="display:block;border-radius:5px;width:125px;height:125px" src='images/ts15.png' id="ts15.png">
								<img style="display:none;border-radius:5px;width:125px;height:125px" src='images/ts15.gif' id="ts15.gif">
							</td>
							<td style="padding-left:1%;vertical-align:middle">
								<p style="line-height:1.6">
									<!-- <a href="https://pdfs.semanticscholar.org/2842/d590524f414f9ebf44ed7e9494ef40d6529f.pdf?_ga=2.57633705.2040626535.1568505530-810167172.1567773220" target="_blank"> -->
										<strong><font size="3">TPT-Dance&#38;Actions : un corpus multimodal d’activités humaines</strong></font>
									<!-- </a> -->
									<br/>
									Aymeric Masurelle,
									<strong>Ahmed Rida Sekkat</strong>,
									Slim Essid,
									Gaël Richard.
									<br/>
									<i>Traitement du Signal 32 (2015): 443-475. </i> 
								</p>
							</td>
						</tr>
					</table>
					<table class="abstract_hide" style="width:100%;padding-left:2%;padding-right:2%;padding-top:1%;max-width:1000px;border:;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" id="ts15_abstract">
						<tr>
							<td style="border-radius:5px;box-shadow:0 2px 4px 0 grey, 0 3px 10px 0 grey;padding-left:2%;padding-right:2%" id="ts15_td">
								<table>
									<tr>
										<td style="vertical-align:middle">
											<p style="text-align: justify">
												<strong>Abstract:</strong> We present a new multimodal database of human activities, TPT - Dance & Actions, for research in multimodal scene analysis and understanding. 
												This corpus focuses on dance scenes (lindy hop, salsa and classical dance), fitness and isolated sequences. 20 dancers and 16 participants were recorded performing respectively 14 
												dance choreographies and 13 sequences of other human activities. These different multimodal scenes have been captured through a variety of media modalities, including video cameras, 
												depth sensors, microphones, piezoelectric transducers and wearable inertial devices (accelerometers, gyroscopes et magnetometers).
											</p>
										</td>
										<td style="padding-left:7px;vertical-align:top">
											<p style="text-align:left">
												[<a href="https://pdfs.semanticscholar.org/2842/d590524f414f9ebf44ed7e9494ef40d6529f.pdf?_ga=2.57633705.2040626535.1568505530-810167172.1567773220" target="_blank">Paper</a>]
											</p>
										</td>
									</tr>
								</table>
							</td>
						</tr>
					</table>
					<br/>
				</td>
			</tr>
		</table>
		<table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tr style="padding:0px">
				<td style="padding-right:10px;padding-left:10px">
					<p>
						<span style="float: right"><font size="-3">&copy; 2018-2022 | Ahmed Rida SEKKAT | <a style="color:#000000" href="https://git.io/sekkat">git.io/sekkat</a></font></span>
						<span style="float: left"><font size="-3">Last update: March 23, 2022</font></span>
					</p>
					<br/><br/>
				</td>
			</tr>
		</table>
		<script>
			function highlight(tab){
				document.getElementById(tab.concat('.png')).style.display = 'none';
				document.getElementById(tab.concat('.gif')).style.display = 'block';
			}
			function lowlight(tab){
				if(document.getElementById(tab.concat('_abstract')).className == "abstract_hide"){
					document.getElementById(tab.concat('.png')).style.display = 'block';
					document.getElementById(tab.concat('.gif')).style.display = 'none';
				}
			}
			function show_hide_abstract(abs){
				if(document.getElementById(abs).className == "abstract_show"){
					document.getElementById(abs).className = "abstract_hide";
					//background-image: linear-gradient(to right, #93aed2, #F8F8FF);
					document.getElementById(abs.replace('_abstract','')).style.backgroundImage = '';
				}
				else{
					document.getElementById(abs).className = "abstract_show";
					document.getElementById(abs.replace('_abstract','').concat('.png')).style.display = 'none';
					document.getElementById(abs.replace('_abstract','').concat('.gif')).style.display = 'block';
					document.getElementById(abs.replace('_abstract','')).style.backgroundImage = 'linear-gradient(to bottom, #93aed2, #F8F8FF)';
					document.getElementById(abs.replace('_abstract','_td')).style.backgroundImage = 'linear-gradient(to right top, #93aed2, #F8F8FF)';
				}
			}
		</script>
	</body>
</html>													